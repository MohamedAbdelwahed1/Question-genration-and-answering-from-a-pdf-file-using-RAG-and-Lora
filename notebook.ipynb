{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-21T12:10:33.459097Z",
     "iopub.status.busy": "2024-03-21T12:10:33.458375Z",
     "iopub.status.idle": "2024-03-21T12:14:50.745674Z",
     "shell.execute_reply": "2024-03-21T12:14:50.744583Z",
     "shell.execute_reply.started": "2024-03-21T12:10:33.459066Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install PyPDF2\n",
    "!pip install -qq -U keras>=3\n",
    "!pip install -qq -U keras-nlp\n",
    "!pip install -U sentence-transformers\n",
    "!pip install -qq -U /kaggle/working/sentence-transformers\n",
    "!pip install -qq -U /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\n",
    "!pip install -qq -U pip ipywidgets jupyter Pyarrow tensorflow-cpu tensorflow-hub tensorflow-text faiss-gpu\n",
    "!cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers/kaggle/working/sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-24T09:43:11.328115Z",
     "iopub.status.busy": "2025-01-24T09:43:11.327275Z",
     "iopub.status.idle": "2025-01-24T09:43:11.332680Z",
     "shell.execute_reply": "2025-01-24T09:43:11.331760Z",
     "shell.execute_reply.started": "2025-01-24T09:43:11.328065Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import faiss\n",
    "import keras\n",
    "import json\n",
    "import PyPDF2\n",
    "import keras_nlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import blingfire as bf\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections.abc import Iterable\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from IPython.display import display, Markdown\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action= 'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-21T12:14:51.089750Z",
     "iopub.status.idle": "2024-03-21T12:14:51.090073Z",
     "shell.execute_reply": "2024-03-21T12:14:51.089923Z",
     "shell.execute_reply.started": "2024-03-21T12:14:51.089910Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text = ''\n",
    "with open('/kaggle/input/pdf-input/P1_7pg_Python_DA_Fabio.pdf', 'rb') as file:\n",
    "    reader_pdf = PyPDF2.PdfReader(file)\n",
    "    for i in range(len(reader_pdf.pages)):\n",
    "        page = reader_pdf.pages[i]\n",
    "        text+= page.extract_text()\n",
    "text = text.replace('\\n', '')\n",
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-21T12:14:51.091574Z",
     "iopub.status.idle": "2024-03-21T12:14:51.091878Z",
     "shell.execute_reply": "2024-03-21T12:14:51.091740Z",
     "shell.execute_reply.started": "2024-03-21T12:14:51.091727Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Loading Instruct Gemma_2b\n",
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_instruct_2b_en\")\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = f'Can u generate 25 questions from this {text}?'\n",
    "answer = gemma_lm.generate(prompt, 6000)\n",
    "questions = list()\n",
    "sentences = answer.split('\\n')\n",
    "for sentence in sentences:\n",
    "    if sentence != '':\n",
    "        starting_of_sentence = sentence[0]\n",
    "        try:\n",
    "            if int(starting_of_sentence):\n",
    "                questions.append(sentence) \n",
    "        except:\n",
    "            pass        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = f'How many relevant questions can you generate from this {text}?'\n",
    "answer = gemma_lm.generate(prompt, 8500)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering after RAG without based on PDF File."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "answers_before_rag = list()\n",
    "for question in tqdm(questions):\n",
    "    answer = gemma_lm.generate(question, max_length= 128)\n",
    "    if len(answer.split('Answer')) >=2:\n",
    "        \n",
    "        answer = answer.split('Answer')[1].replace('*', '')\n",
    "        answer = answer.replace('\\n', '')\n",
    "        answer = answer.replace(':', '')\n",
    "    else:\n",
    "        \n",
    "        answer = answer.replace('\\n', '')\n",
    "        answer = answer.replace(':', '')\n",
    "    answers_before_rag.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(zip(questions, answers_before_rag), columns= ['Questions', 'Answers_before_RAG_without_based_on_PDF_File'])\n",
    "for j, i in tqdm(enumerate(df['Answers_before_RAG_without_based_on_PDF_File'].values)):\n",
    "    if 'What' in i or 'How' in i:\n",
    "        df.loc[j, 'Answers_before_RAG_without_based_on_PDF_File'] = df.loc[j, 'Answers_before_RAG_without_based_on_PDF_File'].split('?')[1]\n",
    "for i in df['Questions']:\n",
    "    df['Questions'] = df['Questions'].str.replace(i[:3], '')\n",
    "df['Questions'] = df['Questions'].str.replace('1', '')\n",
    "df['Questions'] = df['Questions'].str.replace('2', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering after RAG based on PDF File."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Context: {context}\n",
    "\n",
    "System: As a Data analyst, You are gonna asnwer questions based on {pdf_file}.\n",
    "Question: {instruction}\n",
    "\n",
    "Answer: {response}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "answers_based_on_pdf_file = list()\n",
    "for question in tqdm(df['Questions']):\n",
    "    prompt = template.format(\n",
    "        context= \"\",\n",
    "        pdf_file= text,\n",
    "        instruction = question,\n",
    "        response = \"\",\n",
    "    )\n",
    "    answer_based_on_pdf_file = gemma_lm.generate(prompt, max_length= 2056)\n",
    "    answer_based_on_pdf_file = answer_based_on_pdf_file.split('Answer')[-1]\n",
    "    answer_based_on_pdf_file = answer_based_on_pdf_file.replace(':', '')\n",
    "    answer_based_on_pdf_file = answer_based_on_pdf_file.replace(' \\n', '')\n",
    "    answers_based_on_pdf_file.append(answer_based_on_pdf_file)\n",
    "df['Answers_before_RAG_based_on_PDF_File'] = answers_based_on_pdf_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T12:25:58.213801Z",
     "iopub.status.busy": "2024-03-15T12:25:58.212815Z",
     "iopub.status.idle": "2024-03-15T12:25:58.226471Z",
     "shell.execute_reply": "2024-03-15T12:25:58.225578Z",
     "shell.execute_reply.started": "2024-03-15T12:25:58.213761Z"
    }
   },
   "source": [
    "# Question Answering with Wikipedia RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model.max_seq_length = 512\n",
    "sentence_index = faiss.read_index(\"/kaggle/input/wikipedia-2023-07-faiss-index/wikipedia_202307.index\")\n",
    "wiki_files_path = \"/kaggle/input/wikipedia-20230701\"\n",
    "wiki_index_path = f\"{wiki_files_path}/wiki_2023_index.parquet\"\n",
    "batch_size = 64\n",
    "num_sentences_include = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "answers_after_RAG = list()\n",
    "for question in df['Questions']:\n",
    "    #search relevant context\n",
    "    query_embeddings = model.encode(question, batch_size= batch_size, show_progress_bar= True, convert_to_tensor= True, normalize_embeddings= True)\n",
    "    query_embeddings = query_embeddings.detach().cpu().numpy().reshape(1, -1)\n",
    "    search_score, search_index = sentence_index.search(query_embeddings, 10)\n",
    "    search_index = search_index.flatten()\n",
    "    \n",
    "    #get wiki files\n",
    "    wiki_df = pd.read_parquet(wiki_index_path, columns= ['id', 'file'])\n",
    "    wiki_files = wiki_df.iloc[search_index].drop_duplicates().sort_values(['file', 'id']).reset_index(drop=True)\n",
    "\n",
    "    #wiki text\n",
    "    #get wiki text\n",
    "    wiki_text = []\n",
    "    for file in tqdm(wiki_files.file.unique(), total=wiki_files.file.unique().size):\n",
    "        idx = [str(i) for i in wiki_files[wiki_files['file'] == file]['id'].tolist()]\n",
    "        temp_wiki = pd.read_parquet(f\"{wiki_files_path}/{file}\", columns=['id', 'text'])\n",
    "        temp_df = temp_wiki[temp_wiki['id'].isin(idx)].copy()\n",
    "        wiki_text.append(temp_df)\n",
    "    wiki_text = pd.concat(wiki_text).drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    #extract context\n",
    "    wiki_embeddings = model.encode(wiki_text['text'].tolist(),\n",
    "                                                batch_size= batch_size,\n",
    "                                                show_progress_bar= True,\n",
    "                                                convert_to_tensor= False,\n",
    "                                                normalize_embeddings= True)\n",
    "    wiki_embeddings = np.array(wiki_embeddings)\n",
    "    dimension = wiki_embeddings.shape[1]\n",
    "    prompt_index = faiss.IndexFlatL2(dimension)\n",
    "    prompt_index.add(wiki_embeddings)\n",
    "    D, I = prompt_index.search(query_embeddings, num_sentences_include)\n",
    "\n",
    "    contexts = []\n",
    "    for i in I[0]:\n",
    "        context = wiki_text['text'].iloc[i]\n",
    "        contexts.append(context)\n",
    "\n",
    "    contexts = ' '.join(contexts)\n",
    "    \n",
    "    template = \"\"\"\n",
    "    Context: {context}\n",
    "\n",
    "    System: You are the Data Analyst, please answer the questions.\n",
    "    Question: {instruction}\n",
    "\n",
    "    Answer: {response}\n",
    "    \"\"\"\n",
    "    \n",
    "    #process query\n",
    "    query = question\n",
    "    promt_rag = template.format(context=contexts, instruction=query, response=\"\")\n",
    "    answer_after_rag = promt_rag.split('Answer')[-1]\n",
    "    answer_after_rag = answer_after_rag.replace(': ', '')\n",
    "    answer_after_rag = answer_after_rag.replace('\\n', '')\n",
    "    answers_after_RAG.append(answer_after_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df['Answers_after_RAG'] = answers_after_RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "questions = ['1. What is the purpose of data analysis?',\n",
    "             '2. What is the difference between information and data?',\n",
    "             '3. What is data analysis?',\n",
    "             '4. What are the different types of data?',\n",
    "             '5. What is the data analysis process?',\n",
    "             '6. What is the difference between data analysis and model building?',\n",
    "             '7. What is the role of data visualization in data analysis?',\n",
    "             '8. What are the different types of data visualization?',\n",
    "             '9. What is the purpose of data exploration and visualization?',\n",
    "             '10. What is the purpose of predictive modeling?',\n",
    "             '11. How does the predictive power of a model depend on the quality of modeling techniques?',\n",
    "             '12. What is the importance of choosing a good dataset for data analysis?',\n",
    "             '13. What are the preliminary activities of data analysis?',\n",
    "             '14. What is the purpose of data cleaning?',\n",
    "             '15. What is the purpose of data transformation?',\n",
    "             '16. What is the purpose of data exploration and visualization?',\n",
    "             '17. What is the purpose of predictive modeling?',\n",
    "             '18. How does data analysis contribute to professional activities?',\n",
    "             '19. What are the tools and methodologies required for data analysis?',\n",
    "             '20. What is the role of interdisciplinary team members in data analysis?',\n",
    "             '21. What are the different types of categorical data?',\n",
    "             '22. What are the different types of numerical data?',\n",
    "             '23. What is the purpose of data analysis in different fields of applications?',\n",
    "             '24. What is the purpose of data analysis in a world increasingly centralized around information technology?',\n",
    "             '25. What are the challenges and opportunities associated with data analysis?']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('/kaggle/input/databricks-dolly-15k/databricks-dolly-15k.jsonl') as file:\n",
    "    for line in file:\n",
    "        features = json.loads(line)\n",
    "        if features[\"context\"]:\n",
    "            continue\n",
    "        template = \"\"\"\n",
    "        Question: {instruction}\n",
    "\n",
    "        Response: {response}\n",
    "        \"\"\"\n",
    "        data.append(template.format(**features))\n",
    "        \n",
    "data = data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = template.format(\n",
    "    instruction = questions[0],\n",
    "    response = \"\",\n",
    ")\n",
    "print(gemma_lm.generate(prompt, max_length = 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gemma_lm.backbone.enable_lora(rank= 4)\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gemma_lm.preprocessor.sequence_length = 512\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate = 0.0001,\n",
    "    weight_decay = 0.01,\n",
    ")\n",
    "\n",
    "optimizer.exclude_from_weight_decay(var_names= [\"bias\", \"scale\"])\n",
    "\n",
    "gemma_lm.compile(\n",
    "    loss= keras.losses.SparseCategoricalCrossentropy(from_logits= True),\n",
    "    optimizer= optimizer,\n",
    "    weighted_metrics= [keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "gemma_lm.fit(data, epochs= 1, batch_size= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = template.format(\n",
    "    instruction = questions[0],\n",
    "    response = \"\",\n",
    ")\n",
    "print(gemma_lm.generate(prompt, max_length=256))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2893282,
     "sourceId": 4988409,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3122881,
     "sourceId": 5385487,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3521629,
     "sourceId": 6146260,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3524699,
     "sourceId": 6146317,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3526632,
     "sourceId": 6149251,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4583717,
     "sourceId": 7822960,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 3533,
     "modelInstanceId": 5388,
     "sourceId": 11372,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
